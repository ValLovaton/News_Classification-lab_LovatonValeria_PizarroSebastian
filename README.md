# Task 2 ‚Äî Transformer News Classification (AG News + Bonus RPP)

Este repositorio desarrolla y eval√∫a modelos *Transformer* aplicados a la clasificaci√≥n autom√°tica de noticias, utilizando el dataset AG News y un conjunto adicional de 50 noticias reales de RPP Per√∫ como extensi√≥n pr√°ctica (*Bonus Task*).

---

## Objetivo General

Entrenar, comparar y analizar el desempe√±o de modelos basados en *Transformers* (RoBERTa, DeBERTa y BERT Multilingual) en la clasificaci√≥n de noticias seg√∫n cuatro categor√≠as:
**0 - World | 1 - Sports | 2 - Business | 3 - Science/Technology.**

Adem√°s, contrastar los resultados de los modelos con las etiquetas generadas por un LLM aplicado sobre las noticias de RPP, para evaluar coherencia y diferencias sem√°nticas.

---

## Requisitos

* Python ‚â• 3.10
* Librer√≠as principales:
  `transformers`, `datasets`, `torch`, `pandas`, `matplotlib`, `scikit-learn`, `seaborn`
* Instalaci√≥n:

  ```bash
  pip install -r requirements.txt
  ```

---

## Estructura del Repositorio

```
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ rpp_source.json               ‚Üê Noticias de RPP clasificadas
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ agnews_train_eval.ipynb       ‚Üê Carga y preprocesamiento del dataset AG News
‚îÇ   ‚îú‚îÄ‚îÄ agnews_train_eval_step2_roberta.ipynb  ‚Üê Entrenamiento RoBERTa
‚îÇ   ‚îú‚îÄ‚îÄ agnews_train_eval_step3_compare.ipynb  ‚Üê Comparaci√≥n de resultados
‚îÇ   ‚îú‚îÄ‚îÄ bonus_prepare_rpp.ipynb       ‚Üê Clasificaci√≥n LLM y comparaci√≥n (Bonus Task)
‚îÇ
‚îú‚îÄ‚îÄ outputs/                          ‚Üê Resultados y gr√°ficos generados
‚îú‚îÄ‚îÄ src/                              ‚Üê M√≥dulos auxiliares (entrenamiento, m√©tricas, etc.)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Pipeline de Entrenamiento y Evaluaci√≥n

###  Carga y Preparaci√≥n de Datos

* Dataset: `load_dataset("ag_news")`
* Divisi√≥n:

  * 70% entrenamiento
  * 15% validaci√≥n
  * 15% prueba (reservado para evaluaci√≥n final)

### Modelos Entrenados

Se entrenaron tres modelos base en Colab, cada uno con *Hugging Face Transformers*:

| Modelo               | Checkpoint                     | √âpocas | Contexto   |
| -------------------- | ------------------------------ | ------ | ---------- |
| **RoBERTa**          | `roberta-base`                 | 1      | 512 tokens |
| **DeBERTa**          | `microsoft/deberta-v3-base`    | 1      | 512 tokens |
| **BERT Multiling√ºe** | `bert-base-multilingual-cased` | 1      | 512 tokens |

---

## Resultados de Desempe√±o (AG News)

| Modelo                           | F1-score   |
| -------------------------------- | ---------- |
| **roberta-base**                 | 0.7037     |
| **microsoft/deberta-v3-base**    | 0.2917     |
| **bert-base-multilingual-cased** | **0.7833** |

üìà Comparaci√≥n visual:

![F1-score Comparison](outputs/f1_scores_comparison.png)

El modelo **BERT Multiling√ºe** obtuvo el mejor desempe√±o, con un **F1 de 0.78**, superando tanto a RoBERTa (0.70) como a DeBERTa (0.29). Esto sugiere que la cobertura ling√º√≠stica multiling√ºe ayud√≥ a una mejor comprensi√≥n contextual en el dataset.

---

## Bonus Task ‚Äî Clasificaci√≥n de Noticias RPP (LLM vs Transformers)

### **Flujo**

1. Se usaron las 50 noticias m√°s recientes del RSS de RPP Per√∫.
2. Un LLM (ChatGPT API) clasific√≥ cada noticia seg√∫n las categor√≠as AG News.
3. Los tres modelos entrenados aplicaron inferencia sobre las mismas noticias.
4. Se compararon los resultados (predicciones vs LLM) mediante F1-score.

### **Resultados**

| Comparaci√≥n          | Observaciones                                                                                       |
| -------------------- | --------------------------------------------------------------------------------------------------- |
| **BERT Multiling√ºe** | Mostr√≥ mayor coherencia con las etiquetas del LLM. Capta mejor el lenguaje period√≠stico y regional. |
| **RoBERTa**          | Aceptable, aunque tiende a confundir temas de ciencia y negocios.                                   |
| **DeBERTa**          | Peor desempe√±o: bajo ajuste en textos fuera del dominio original ingl√©s.                            |

**Conclusi√≥n del Bonus:**
El modelo BERT Multiling√ºe logra la mayor consistencia con las clasificaciones del LLM, lo que confirma su capacidad para manejar contextos y estilos de texto no estrictamente anglosajones, como las noticias peruanas.

---

## M√©tricas Principales

* **M√©trica usada:** Macro F1-score (balancea clases desiguales)
* **Batch size:** 8
* **Optimizer:** AdamW
* **Learning rate:** 5e-5
* **Seed reproducible:** 42
* **Checkpoints guardados:** en `/outputs`
---

## Discusi√≥n e Interpretaci√≥n

El desempe√±o diferencial entre los modelos se explica por:

* **Dominio del pre-entrenamiento:** RoBERTa y DeBERTa fueron preentrenados en ingl√©s, lo que reduce su comprensi√≥n sem√°ntica para textos con estructura mixta o hispana.
* **BERT Multiling√ºe** muestra una mejor generalizaci√≥n a contextos diversos, y por ello logra una clasificaci√≥n m√°s estable tanto en AG News como en RPP.
* Las divergencias con el LLM se deben a diferencias de contexto y longitudes de texto (los modelos locales procesan tokens truncados, mientras que el LLM conserva contexto completo).

---

## Reproducibilidad

Para ejecutar todo el pipeline en Colab:

```bash
!git clone https://github.com/ValLovaton/agnews-transformers-lab_LovatonValeria_PizarroSebastian.git
%cd agnews-transformers-lab_LovatonValeria_PizarroSebastian
!pip install -r requirements.txt
```

Abrir y correr secuencialmente los notebooks de `/notebooks/`:

1. `agnews_train_eval.ipynb`
2. `agnews_train_eval_step2_roberta.ipynb`
3. `agnews_train_eval_step3_compare.ipynb`
4. `bonus_prepare_rpp.ipynb` *(para el Bonus Task con RPP)*

---

## Conclusi√≥n

El proyecto demuestra que los modelos *Transformer* pueden adaptarse de manera efectiva a la clasificaci√≥n de noticias, y que los modelos multiling√ºes ofrecen una ventaja clara al aplicarse a fuentes en espa√±ol.
La comparaci√≥n con un LLM permiti√≥ validar la coherencia sem√°ntica de los resultados y resaltar las limitaciones de los modelos entrenados en dominios monoling√ºes.


¬øQuieres que te agregue tambi√©n una **versi√≥n corta (resumen ejecutivo)** al final del README para defensa oral o repositorio p√∫blico (como un p√°rrafo resumen tipo abstract)?
